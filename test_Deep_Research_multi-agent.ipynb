{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b80708f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 16:28:14,604 - INFO - [src.agents.agents] [System] Agent logging setup complete. Root logger level set to DEBUG.\n"
     ]
    }
   ],
   "source": [
    "# filepath: test_Deep_Research_multi-agent.ipynb\n",
    "\n",
    "# Cell 1: Imports and Logging\n",
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from typing import Any, Optional, Dict, List\n",
    "from src.agents.agents import (\n",
    "    Agent,\n",
    "    RequestContext,\n",
    "    LogLevel,\n",
    "    Message,\n",
    "    init_agent_logging,\n",
    ")  # Added setup_agent_logging\n",
    "from src.models.models import ModelConfig\n",
    "\n",
    "# NEW IMPORTS for real search tools\n",
    "from googlesearch import search as google_search_lib  # Alias to avoid conflict\n",
    "from semanticscholar import (\n",
    "    SemanticScholar as S2API,\n",
    ")  # Renaming to avoid conflict if 'scholarly' is also imported\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd  # For timestamp formatting in progress_monitor\n",
    "\n",
    "\n",
    "# --- Logging Configuration ---\n",
    "# Call the centralized setup function to configure logging for the entire application/notebook.\n",
    "# This function handles setting up formatters, filters, and handlers.\n",
    "# - `level`: Sets the root logger level (e.g., logging.INFO, logging.DEBUG).\n",
    "# - `clear_existing_handlers`: True by default, useful in notebooks to prevent\n",
    "#   duplicate log messages if this cell is re-run.\n",
    "init_agent_logging(level=logging.DEBUG, clear_existing_handlers=True)\n",
    "\n",
    "\n",
    "# --- Notebook-Specific Logger (Optional) ---\n",
    "# It can be useful to have a specific logger for messages originating directly from notebook operations,\n",
    "# distinct from agent or library logs, though here it will also use the root logger's handlers\n",
    "# and level settings established by setup_agent_logging().\n",
    "notebook_logger = logging.getLogger(\"DeepResearchNotebook\")\n",
    "# The level for this specific logger can be set independently if needed,\n",
    "# but it will not output messages below the root logger's level.\n",
    "# For instance, if root is INFO, setting this to DEBUG won't show its DEBUG messages\n",
    "# unless the root logger is also set to DEBUG.\n",
    "# notebook_logger.setLevel(logging.DEBUG) # Example: if you want this logger to be more verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9899c887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 16:28:17,367 - DEBUG - [DefaultLogger] [System] Read API key for provider 'openai' from env var 'OPENAI_API_KEY'.\n",
      "2025-05-19 16:28:17,368 - DEBUG - [DefaultLogger] [System] Read API key for provider 'openai' from env var 'OPENAI_API_KEY'.\n",
      "2025-05-19 16:28:17,370 - INFO - [DefaultLogger] [OrchestratorAgent] Agent registered: OrchestratorAgent (Class: Agent)\n",
      "2025-05-19 16:28:17,371 - INFO - [DefaultLogger] [RetrievalAgent] Agent registered: RetrievalAgent (Class: Agent)\n",
      "2025-05-19 16:28:17,371 - INFO - [DefaultLogger] [ResearcherAgent] Agent registered: ResearcherAgent (Class: Agent)\n",
      "2025-05-19 16:28:17,372 - INFO - [DefaultLogger] [SynthesizerAgent] Agent registered: SynthesizerAgent (Class: Agent)\n",
      "2025-05-19 16:28:17,372 - INFO - [DeepResearchNotebook] [System] All agents initialized.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configurations, Tool Definitions, Agent Descriptions & Initialization\n",
    "\n",
    "# --- Model Configurations using ModelConfig ---\n",
    "try:\n",
    "    model_config_capable = ModelConfig(\n",
    "        type=\"api\",\n",
    "        provider=\"openai\",\n",
    "        name=\"gpt-4-turbo\",\n",
    "        temperature=0.3,\n",
    "        max_tokens=4000,\n",
    "    )\n",
    "    model_config_worker = ModelConfig(\n",
    "        type=\"api\",\n",
    "        provider=\"openai\",\n",
    "        name=\"gpt-4.1-mini\",\n",
    "        temperature=0.1,\n",
    "        max_tokens=4000,\n",
    "    )\n",
    "except ValueError as e:\n",
    "    notebook_logger.error(\n",
    "        f\"Failed to create ModelConfig: {e}. Ensure API keys are set (e.g., OPENAI_API_KEY).\"\n",
    "    )\n",
    "    raise\n",
    "\n",
    "# --- Search Tools ---\n",
    "\n",
    "\n",
    "def tool_google_search_api(query: str, num_results: int = 3, lang: str = \"en\") -> str:\n",
    "    notebook_logger.info(\n",
    "        f\"Tool Google Custom Search API for: {query}\", extra={\"agent_name\": \"Tool\"}\n",
    "    )\n",
    "\n",
    "    api_key = os.getenv(\"GOOGLE_SEARCH_API_KEY\")\n",
    "    cse_id = os.getenv(\"GOOGLE_CSE_ID_GENERIC\")  # Make sure this env var is set\n",
    "\n",
    "    if not api_key:\n",
    "        notebook_logger.error(\n",
    "            \"GOOGLE_SEARCH_API_KEY not found in environment variables.\",\n",
    "            extra={\"agent_name\": \"Tool\"},\n",
    "        )\n",
    "        return json.dumps({\"error\": \"Google Search API key not configured.\"})\n",
    "    if not cse_id:\n",
    "        notebook_logger.error(\n",
    "            \"GOOGLE_CSE_ID_GENERIC not found in environment variables.\",\n",
    "            extra={\"agent_name\": \"Tool\"},\n",
    "        )\n",
    "        return json.dumps(\n",
    "            {\"error\": \"Google Custom Search Engine ID (CX) not configured.\"}\n",
    "        )\n",
    "\n",
    "    results = []\n",
    "    response_obj = None\n",
    "    try:\n",
    "        url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "        params = {\n",
    "            \"key\": api_key,\n",
    "            \"cx\": cse_id,\n",
    "            \"q\": query,\n",
    "            \"num\": num_results,\n",
    "            \"lr\": f\"lang_{lang}\",\n",
    "        }\n",
    "\n",
    "        response_obj = requests.get(url, params=params, timeout=10)\n",
    "        response_obj.raise_for_status()\n",
    "\n",
    "        search_data = response_obj.json()\n",
    "\n",
    "        if \"items\" not in search_data:\n",
    "            notebook_logger.info(\n",
    "                f\"Tool Google Custom Search API for '{query}' returned no items.\",\n",
    "                extra={\"agent_name\": \"Tool\"},\n",
    "            )\n",
    "            return json.dumps([])\n",
    "\n",
    "        for item in search_data.get(\"items\", []):\n",
    "            title = item.get(\"title\", \"N/A\")\n",
    "            link = item.get(\"link\", \"N/A\")\n",
    "            snippet = item.get(\"snippet\", \"No snippet available.\")\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"title\": title,\n",
    "                    \"content\": snippet,\n",
    "                    \"source\": \"Google Custom Search API\",\n",
    "                    \"url\": link,\n",
    "                }\n",
    "            )\n",
    "            if len(results) >= num_results:\n",
    "                break\n",
    "\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        error_details = \"No response object\"\n",
    "        if response_obj is not None:\n",
    "            try:\n",
    "                error_details = response_obj.json()\n",
    "            except json.JSONDecodeError:\n",
    "                error_details = response_obj.text\n",
    "        notebook_logger.error(\n",
    "            f\"Google Custom Search API HTTP error: {http_err} - Response: {error_details}\",\n",
    "            extra={\"agent_name\": \"Tool\"},\n",
    "        )\n",
    "        return json.dumps(\n",
    "            {\n",
    "                \"error\": f\"Google Custom Search API HTTP error: {http_err}\",\n",
    "                \"details\": error_details,\n",
    "            }\n",
    "        )\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        notebook_logger.error(\n",
    "            f\"Tool Google Custom Search API request failed: {e}\",\n",
    "            extra={\"agent_name\": \"Tool\"},\n",
    "        )\n",
    "        return json.dumps(\n",
    "            {\"error\": f\"Tool Google Custom Search API request failed: {str(e)}\"}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        notebook_logger.error(\n",
    "            f\"Tool Google Custom Search API failed: {e}\",\n",
    "            exc_info=True,\n",
    "            extra={\"agent_name\": \"Tool\"},\n",
    "        )\n",
    "        return json.dumps({\"error\": f\"Tool Google Custom Search API failed: {str(e)}\"})\n",
    "\n",
    "    if not results:\n",
    "        notebook_logger.info(\n",
    "            f\"Tool Google Custom Search API for '{query}' returned no results after processing.\",\n",
    "            extra={\"agent_name\": \"Tool\"},\n",
    "        )\n",
    "        return json.dumps([])\n",
    "\n",
    "    return json.dumps(results)\n",
    "\n",
    "\n",
    "def tool_google_search_community(\n",
    "    query: str, num_results: int = 3, lang: str = \"en\"\n",
    ") -> str:\n",
    "    notebook_logger.info(\n",
    "        f\"Tool Google Search (Community Library) for: {query}\",\n",
    "        extra={\"agent_name\": \"Tool\"},\n",
    "    )\n",
    "    results = []\n",
    "    try:\n",
    "        search_results = list(\n",
    "            google_search_lib(\n",
    "                query, num_results=num_results, lang=lang, sleep_interval=1\n",
    "            )\n",
    "        )\n",
    "        for url_item in search_results:\n",
    "            title = \"N/A\"\n",
    "            content_snippet = \"Could not retrieve content.\"\n",
    "            try:\n",
    "                headers = {\n",
    "                    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "                }\n",
    "                page = requests.get(url_item, headers=headers, timeout=10)\n",
    "                page.raise_for_status()\n",
    "                soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "                title_tag = soup.find(\"title\")\n",
    "                if title_tag and title_tag.string:\n",
    "                    title = title_tag.string.strip()\n",
    "\n",
    "                meta_description = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "                if meta_description and meta_description.get(\"content\"):\n",
    "                    content_snippet = meta_description.get(\"content\").strip()\n",
    "                else:\n",
    "                    paragraphs = soup.find_all(\"p\")\n",
    "                    text_content = \" \".join(\n",
    "                        [p.get_text().strip() for p in paragraphs[:3]]\n",
    "                    )\n",
    "                    if text_content:\n",
    "                        content_snippet = text_content[:500] + (\n",
    "                            \"...\" if len(text_content) > 500 else \"\"\n",
    "                        )\n",
    "                    elif soup.body:\n",
    "                        content_snippet = soup.body.get_text(separator=\" \", strip=True)[\n",
    "                            :500\n",
    "                        ] + (\n",
    "                            \"...\"\n",
    "                            if len(soup.body.get_text(separator=\" \", strip=True)) > 500\n",
    "                            else \"\"\n",
    "                        )\n",
    "\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"title\": title,\n",
    "                        \"content\": content_snippet,\n",
    "                        \"source\": \"Google Search (Community Library)\",\n",
    "                        \"url\": url_item,\n",
    "                    }\n",
    "                )\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                notebook_logger.warning(\n",
    "                    f\"Failed to fetch URL {url_item}: {e}\", extra={\"agent_name\": \"Tool\"}\n",
    "                )\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"title\": f\"Error fetching {url_item}\",\n",
    "                        \"content\": str(e),\n",
    "                        \"source\": \"Google Search (Community Library)\",\n",
    "                        \"url\": url_item,\n",
    "                        \"error\": True,\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e_parse:\n",
    "                notebook_logger.warning(\n",
    "                    f\"Failed to parse content from {url_item}: {e_parse}\",\n",
    "                    extra={\"agent_name\": \"Tool\"},\n",
    "                )\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"title\": f\"Error parsing {url_item}\",\n",
    "                        \"content\": str(e_parse),\n",
    "                        \"source\": \"Google Search (Community Library)\",\n",
    "                        \"url\": url_item,\n",
    "                        \"error\": True,\n",
    "                    }\n",
    "                )\n",
    "            if len(results) >= num_results:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        notebook_logger.error(\n",
    "            f\"Tool Google Search (Community Library) failed: {e}\",\n",
    "            exc_info=True,\n",
    "            extra={\"agent_name\": \"Tool\"},\n",
    "        )\n",
    "        return json.dumps(\n",
    "            {\"error\": f\"Tool Google Search (Community Library) failed: {str(e)}\"}\n",
    "        )\n",
    "\n",
    "    if not results:\n",
    "        notebook_logger.info(\n",
    "            f\"Tool Google Search (Community Library) for '{query}' returned no results.\",\n",
    "            extra={\"agent_name\": \"Tool\"},\n",
    "        )\n",
    "        return json.dumps([])\n",
    "    return json.dumps(results)\n",
    "\n",
    "\n",
    "# def tool_semantic_scholar_search(query: str, limit: int = 5) -> str:\n",
    "#     \"\"\"\n",
    "#     Performs a search for academic papers using the Semantic Scholar API.\n",
    "\n",
    "#     Args:\n",
    "#         query: The search query string.\n",
    "#         limit: The maximum number of results to return.\n",
    "\n",
    "#     Returns:\n",
    "#         A JSON string representing a list of search results,\n",
    "#         where each result is a dictionary with 'title', 'content' (abstract), 'source', and 'url'.\n",
    "#         Returns an empty JSON list '[]' if no results are found or a JSON string with an error message if an error occurs.\n",
    "#     \"\"\"\n",
    "#     from semanticscholar import SemanticScholar\n",
    "#     sch = SemanticScholar()\n",
    "#     results = []\n",
    "#     try:\n",
    "#         print(f\"Searching Semantic Scholar for: {query} (limit: {limit})\")\n",
    "#         papers = sch.search_paper(query, limit=limit)\n",
    "\n",
    "#         count = 0\n",
    "#         for paper in papers:\n",
    "#             if count >= limit:\n",
    "#                 break\n",
    "\n",
    "#             title = paper.title if hasattr(paper, 'title') else \"N/A\"\n",
    "#             content = paper.abstract if hasattr(paper, 'abstract') and paper.abstract else \"Abstract not available.\"\n",
    "#             url = paper.url if hasattr(paper, 'url') else \"URL not available\"\n",
    "\n",
    "#             results.append({\n",
    "#                 \"title\": title,\n",
    "#                 \"content\": content,\n",
    "#                 \"source\": \"Semantic Scholar Search Tool\",\n",
    "#                 \"url\": url\n",
    "#             })\n",
    "#             count += 1\n",
    "\n",
    "#         print(f\"Found {len(results)} results from Semantic Scholar.\")\n",
    "#         return json.dumps(results)\n",
    "#     except Exception as e:\n",
    "#         error_message = f\"Error during Semantic Scholar search: {str(e)}\"\n",
    "#         print(error_message)\n",
    "#         return json.dumps({\"error\": error_message, \"source\": \"Semantic Scholar Search Tool\"})\n",
    "\n",
    "\n",
    "# --- Tool Schemas ---\n",
    "google_search_api_schema = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"tool_google_search_api\",\n",
    "            \"description\": \"Performs a Google web search using the official Custom Search API for a given query and returns top results with snippets.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\"type\": \"string\", \"description\": \"The search query.\"},\n",
    "                    \"num_results\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"Number of results to return (default 3).\",\n",
    "                    },\n",
    "                    \"lang\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Language for search (e.g., 'en', 'es', default 'en').\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"query\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "google_search_community_schema = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"tool_google_search_community\",\n",
    "            \"description\": \"Performs a Google web search using a community library for a given query and returns top results with snippets by scraping/parsing.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\"type\": \"string\", \"description\": \"The search query.\"},\n",
    "                    \"num_results\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"Number of results to return (default 3).\",\n",
    "                    },\n",
    "                    \"lang\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Language for search (e.g., 'en', 'es', default 'en').\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"query\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "# semantic_scholar_search_schema = [\n",
    "#     {\"type\": \"function\", \"function\": {\n",
    "#         \"name\": \"tool_semantic_scholar_search\",\n",
    "#         \"description\": \"Performs an academic paper search using the Semantic Scholar API and returns top results with abstracts and metadata.\",\n",
    "#         \"parameters\": {\"type\": \"object\", \"properties\": {\n",
    "#             \"query\": {\"type\": \"string\", \"description\": \"The search query for academic papers.\"},\n",
    "#             \"num_results\": {\"type\": \"integer\", \"description\": \"Number of results to return (default 3).\"}\n",
    "#         }, \"required\": [\"query\"]}\n",
    "#     }}\n",
    "# ]\n",
    "\n",
    "retrieval_tools = {\n",
    "    \"tool_google_search_api\": tool_google_search_api,\n",
    "    \"tool_google_search_community\": tool_google_search_community,\n",
    "    # \"tool_semantic_scholar_search\": tool_semantic_scholar_search\n",
    "}\n",
    "retrieval_tools_schema = (\n",
    "    google_search_api_schema + google_search_community_schema\n",
    ")  # + semantic_scholar_search_schema\n",
    "\n",
    "# --- Agent Descriptions ---\n",
    "ORCHESTRATOR_DESCRIPTION = \"\"\"\n",
    "You are a meticulous Research Orchestrator. Your goal is to manage a team of agents (RetrievalAgent, ResearcherAgent, SynthesizerAgent) to answer a user's complex query thoroughly.\n",
    "You will decide on a `next_action` (from 'invoke_agent', 'call_tool', 'final_response') and provide necessary input for it. The system will guide you on the exact JSON structure for your decisions.\n",
    "\n",
    "**Core Task:** To answer a user's query by coordinating other agents.\n",
    "\n",
    "**Orchestration Steps:**\n",
    "1.  **Analyze Query & Plan:** Understand the user's request. Formulate a high-level plan. Identify the first sub-question for the `RetrievalAgent`.\n",
    "2.  **Delegate Retrieval:** For a sub-question, invoke the `RetrievalAgent`, providing it with the search query string.\n",
    "3.  **Manage Information:**\n",
    "    *   The `RetrievalAgent` will return a `Message` object. Its `content` field will contain a JSON string, which is a list of search result objects (each having 'title', 'content', 'source', 'url').\n",
    "    *   **Crucially, note the `message_id` of this `RetrievalAgent`'s response.** This ID is your key to accessing the actual retrieved data later.\n",
    "    *   Parse the `RetrievalAgent`'s JSON content for your understanding.\n",
    "4.  **Delegate Research/Validation:**\n",
    "    *   For the current sub-question, invoke the `ResearcherAgent`.\n",
    "    *   Provide it with the sub-question to validate and, critically, use the `context_message_ids` parameter to pass the `message_id` (from Step 3) of the `RetrievalAgent`'s `Message` that contains the data to be validated.\n",
    "5.  **Iterate & Collect Validated Data:**\n",
    "    *   The `ResearcherAgent` will return a `Message`. Its `content` will be a JSON string indicating `status` (e.g., \"sufficient\", \"insufficient\") and `reason`.\n",
    "    *   If `status` is \"sufficient\":\n",
    "        a.  **Retrieve Original Data:** Use the `message_id` you noted in Step 3 (and passed to `ResearcherAgent` in Step 4) to access the `RetrievalAgent`'s original `Message` from your memory.\n",
    "        b.  **Parse Original Data:** The `content` of that `RetrievalAgent`'s `Message` is a JSON string. Parse it to get the list of original search result objects (dictionaries with 'title', 'content', 'source', 'url').\n",
    "        c.  **Store Validated Documents:** Store these complete search result objects. These are your \"validated source documents\" for this sub-question.\n",
    "    *   If `status` is \"insufficient\", refine the search query for that sub-question and go back to Step 2 (Delegate Retrieval).\n",
    "6.  **Synthesize Report:**\n",
    "    *   Once all critical sub-questions have \"validated source documents\" (as collected in Step 5c):\n",
    "    *   Compile a single list containing *all* these \"validated source document\" objects (the dictionaries with 'title', 'content', 'source', 'url') from all successfully validated sub-questions.\n",
    "    *   Invoke the `SynthesizerAgent`. Provide it with the original user query and this compiled list of \"validated source document\" objects.\n",
    "7.  **Final Output:**\n",
    "    *   The `SynthesizerAgent` will return a `Message` containing the final report in Markdown format.\n",
    "    *   Your `next_action` should be `final_response`, providing this Markdown report as the output.\n",
    "\n",
    "**Constraints:**\n",
    "*   When invoking agents and referencing past messages, correctly use `message_id`s from `Message` objects in your memory.\n",
    "*   If interaction or depth limits are approached, try to synthesize a report with the information gathered so far.\n",
    "\"\"\"\n",
    "\n",
    "RETRIEVAL_DESCRIPTION = \"\"\"\n",
    "You are a Retrieval Agent. Your task is to find information relevant to a specific query string given to you as your main input, using available search tools (tool_google_search_api, tool_google_search_community, tool_semantic_scholar_search).\n",
    "\n",
    "Your Process:\n",
    "1.  Analyze the specific query string.\n",
    "2.  Choose the most appropriate tool(s).\n",
    "3.  If you decide to use a tool, indicate this by populating the `tool_calls` field in your response. Ensure the arguments string for the tool is a valid JSON string.\n",
    "4.  After tool execution, the system will feed you back the results as a `role='tool'` message.\n",
    "5.  Based on these results, you might refine your search and call tools again, or if you have sufficient information, proceed to step 6.\n",
    "6.  Your final output for this task should be a JSON list string of the gathered results. Each item in the list should be a dictionary with keys: \"title\", \"content\", \"source\", \"url\".\n",
    "    Example: `[{\"title\": \"...\", \"content\": \"...\", \"source\": \"Semantic Scholar API\", \"url\": \"...\", \"authors\": \"...\", \"year\": \"...\"}]`\n",
    "    If no relevant information is found, return an empty JSON list string `'[]'`. If a tool fails, its error will be in the tool result message; you should then formulate a final JSON string, perhaps indicating the error, e.g., `'{\"error\": \"Search failure during retrieval\"}'`.\n",
    "\"\"\"\n",
    "\n",
    "RESEARCHER_DESCRIPTION = \"\"\"\n",
    "You are a critical Researcher Agent. Your task is to evaluate if information, provided via context messages, *sufficiently and relevantly* answers a *specific sub_question*.\n",
    "\n",
    "Your Input:\n",
    "1.  Your main input will be a JSON string containing a `\\\"sub_question\\\"`.\n",
    "2.  Referenced context messages (from `RetrievalAgent`) will be in your memory. Their `content` is a JSON string (a list of information snippets with 'title', 'content', 'source', 'url').\n",
    "\n",
    "Your Process:\n",
    "1.  Parse your main input JSON to get the `sub_question`.\n",
    "2.  Retrieve and parse the JSON data from the `content` of the referenced context message(s).\n",
    "3.  Analyze each information snippet for relevance to the `sub_question`.\n",
    "4.  Assess if the *combined* relevant information is *sufficient* to answer the `sub_question` thoroughly.\n",
    "5.  Your final output must be a JSON object string representing your assessment.\n",
    "    *   Example Sufficient: `{\"status\": \"sufficient\", \"reason\": \"Information adequately addresses X and Y.\"}`\n",
    "    *   Example Insufficient: `{\"status\": \"insufficient\", \"reason\": \"Lacks details on Z.\", \"missing_info_request\": \"Need specific examples of Z.\"}`\n",
    "    *   Example Irrelevant: `{\"status\": \"irrelevant\", \"reason\": \"Discusses A, but question was about B.\"}`\n",
    "\"\"\"\n",
    "\n",
    "SYNTHESIZER_DESCRIPTION = \"\"\"\n",
    "You are a Synthesizer Agent. Your task is to write a comprehensive, well-structured report answering an original user query, based *only* on validated information items provided to you.\n",
    "\n",
    "Your Input:\n",
    "1.  Your main input will be a JSON string containing:\n",
    "    *   `\\\"user_query\\\"`: The original user query string.\n",
    "    *   `\\\"validated_data\\\"`: A list of validated information items. Each item is a dictionary with 'title', 'content', 'source', and 'url'.\n",
    "\n",
    "Your Process:\n",
    "1.  Parse your main input JSON to get the `user_query` and `validated_data`.\n",
    "2.  Deeply analyze the `user_query`.\n",
    "3.  Review all `validated_data`, paying attention to content and source URLs.\n",
    "4.  Formulate a structure for the report.\n",
    "5.  Write a clear, coherent, and comprehensive report **in Markdown format**.\n",
    "6.  Ground your report *strictly* in the `validated_data`.\n",
    "7.  **Include references to the sources using Markdown links (e.g., `[Source Title](URL)` or footnotes like `[1]` with a corresponding reference list) where appropriate.**\n",
    "8.  Your final output for this task should be the final report as a **Markdown string**. Start the report directly.\n",
    "\"\"\"\n",
    "\n",
    "# --- Agent Initialization ---\n",
    "orchestrator_agent = Agent(\n",
    "    agent_name=\"OrchestratorAgent\",\n",
    "    model_config=model_config_capable,\n",
    "    description=ORCHESTRATOR_DESCRIPTION,\n",
    "    allowed_peers=[\"RetrievalAgent\", \"ResearcherAgent\", \"SynthesizerAgent\"],\n",
    "    memory_type=\"conversation_history\",\n",
    ")\n",
    "\n",
    "retrieval_agent = Agent(\n",
    "    agent_name=\"RetrievalAgent\",\n",
    "    model_config=model_config_worker,\n",
    "    description=RETRIEVAL_DESCRIPTION,\n",
    "    tools=retrieval_tools,\n",
    "    tools_schema=retrieval_tools_schema,\n",
    "    allowed_peers=[],\n",
    "    memory_type=\"conversation_history\",\n",
    ")\n",
    "\n",
    "researcher_agent = Agent(\n",
    "    agent_name=\"ResearcherAgent\",\n",
    "    model_config=model_config_worker,\n",
    "    description=RESEARCHER_DESCRIPTION,\n",
    "    allowed_peers=[],\n",
    "    memory_type=\"conversation_history\",\n",
    ")\n",
    "\n",
    "synthesizer_agent = Agent(\n",
    "    agent_name=\"SynthesizerAgent\",\n",
    "    model_config=model_config_capable,\n",
    "    description=SYNTHESIZER_DESCRIPTION,\n",
    "    allowed_peers=[],\n",
    "    memory_type=\"conversation_history\",\n",
    ")\n",
    "notebook_logger.info(\"All agents initialized.\", extra={\"agent_name\": \"System\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a5139a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 16:28:19,290 - INFO - [DeepResearchNotebook] [Tool] Tool Google Custom Search API for: AI multi agent systems\n",
      "2025-05-19 16:28:19,293 - DEBUG - [urllib3.connectionpool] [System] Starting new HTTPS connection (1): www.googleapis.com:443\n",
      "2025-05-19 16:28:19,709 - DEBUG - [urllib3.connectionpool] [System] https://www.googleapis.com:443 \"GET /customsearch/v1?key=AIzaSyABCoDRXZktWguIbva_XVZHaCHskLr6UN4&cx=c681df779dee54fbd&q=AI+multi+agent+systems&num=10&lr=lang_en HTTP/1.1\" 200 None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'What is a Multiagent System? | IBM',\n",
       "  'content': 'A multiagent system (MAS) consists of multiple artificial intelligence (AI) agents working collectively to perform tasks on behalf of a user or another system.',\n",
       "  'source': 'Google Custom Search API',\n",
       "  'url': 'https://www.ibm.com/think/topics/multiagent-system'},\n",
       " {'title': 'Exploring Multi-Agent AI Systems',\n",
       "  'content': 'Sep 3, 2024 ... AI agents can answer questions and take actions even without being explicitly programmed to do so. Lately, the concept of multi-agent systems is gaining\\xa0...',\n",
       "  'source': 'Google Custom Search API',\n",
       "  'url': 'https://techcommunity.microsoft.com/blog/aiplatformblog/the-future-of-ai-exploring-multi-agent-ai-systems/4226593'},\n",
       " {'title': 'Build and manage multi-system agents with Vertex AI | Google ...',\n",
       "  'content': 'Apr 9, 2025 ... Vertex AI – our comprehensive platform to orchestrate the three pillars of production AI: models, data, and agents – seamlessly brings these elements together.',\n",
       "  'source': 'Google Custom Search API',\n",
       "  'url': 'https://cloud.google.com/blog/products/ai-machine-learning/build-and-manage-multi-system-agents-with-vertex-ai'},\n",
       " {'title': 'Multi-agent systems | The Alan Turing Institute',\n",
       "  'content': 'Multi-agent systems (MAS) are a core area of research of contemporary artificial intelligence. A multi-agent system consists of multiple decision-making\\xa0...',\n",
       "  'source': 'Google Custom Search API',\n",
       "  'url': 'https://www.turing.ac.uk/research/interest-groups/multi-agent-systems'},\n",
       " {'title': 'What is a Multi Agent System - Relevance AI',\n",
       "  'content': 'A multi-agent system (MAS) is composed of multiple interacting intelligent agents - autonomous entities that can sense, learn models of their environment, make\\xa0...',\n",
       "  'source': 'Google Custom Search API',\n",
       "  'url': 'https://relevanceai.com/learn/what-is-a-multi-agent-system'},\n",
       " {'title': 'Do We Actually Need Multi-Agent AI Systems? : r/AI_Agents',\n",
       "  'content': \"Mar 12, 2025 ... Multi agent systems should only be implemented only when a single agent can't perform the task assigned. So many people jump into multi agent\\xa0...\",\n",
       "  'source': 'Google Custom Search API',\n",
       "  'url': 'https://www.reddit.com/r/AI_Agents/comments/1j9bwl7/do_we_actually_need_multiagent_ai_systems/'},\n",
       " {'title': 'Multi AI Agent Systems with crewAI - DeepLearning.AI',\n",
       "  'content': 'About this course · Role-playing: Assign specialized roles to agents · Memory: Provide agents with short-term, long-term, and shared memory · Tools: Assign pre-\\xa0...',\n",
       "  'source': 'Google Custom Search API',\n",
       "  'url': 'https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/'},\n",
       " {'title': 'Building Complex Multi-Agent Systems : r/AI_Agents',\n",
       "  'content': \"Jan 3, 2025 ... 35 votes, 24 comments. Hi all, As someone who leads an AI eng team and builds agents professionally, I've been exploring how to scale\\xa0...\",\n",
       "  'source': 'Google Custom Search API',\n",
       "  'url': 'https://www.reddit.com/r/AI_Agents/comments/1hsnbgf/building_complex_multiagent_systems/'},\n",
       " {'title': 'The Promise of Multi-Agent AI and AutoGen',\n",
       "  'content': 'May 24, 2024 ... By allowing agents to work together, critique one another, and share their insights, the system can develop a more comprehensive understanding\\xa0...',\n",
       "  'source': 'Google Custom Search API',\n",
       "  'url': 'https://www.forbes.com/sites/joannechen/2024/05/24/the-promise-of-multi-agent-ai/'},\n",
       " {'title': 'Multi-agent system - Wikipedia',\n",
       "  'content': 'A multi-agent system (MAS or \"self-organized system\") is a computerized system composed of multiple interacting intelligent agents.',\n",
       "  'source': 'Google Custom Search API',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Multi-agent_system'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "output = tool_google_search_api(query=\"AI multi agent systems\", num_results=10)\n",
    "json.loads(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bbcd3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# output = tool_semantic_scholar_search(query=\"AI multi agent systems\", limit=10)\n",
    "# json.loads(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1763235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Main Execution Logic\n",
    "async def run_deep_research_task(user_query: str, max_orchestrator_steps: int = 15):\n",
    "    task_id = f\"deep-research-{uuid.uuid4()}\"\n",
    "    progress_queue = asyncio.Queue()\n",
    "\n",
    "    request_context = RequestContext(\n",
    "        task_id=task_id,\n",
    "        initial_prompt=user_query,\n",
    "        progress_queue=progress_queue,\n",
    "        log_level=LogLevel.DETAILED,\n",
    "        max_depth=3,\n",
    "        max_interactions=(max_orchestrator_steps * 3) + 5,\n",
    "    )\n",
    "\n",
    "    _notebook_logger = logging.getLogger(\n",
    "        \"DeepResearchNotebook\"\n",
    "    )  # Use the notebook's logger\n",
    "\n",
    "    async def progress_monitor(q: asyncio.Queue):\n",
    "        while True:\n",
    "            update = await q.get()\n",
    "            if update is None:\n",
    "                q.task_done()\n",
    "                break\n",
    "\n",
    "            log_message_parts = [\n",
    "                f\"{pd.Timestamp(update.timestamp, unit='s')}\",\n",
    "                f\"LVL {update.level.value}\",\n",
    "                f\"[{update.agent_name or 'System'}]\",\n",
    "                update.message,\n",
    "            ]\n",
    "            if update.data:\n",
    "                try:\n",
    "                    log_message_parts.append(f\"Data: {json.dumps(update.data)}\")\n",
    "                except TypeError:\n",
    "                    log_message_parts.append(\n",
    "                        f\"Data: (Unserializable data: {type(update.data)})\"\n",
    "                    )\n",
    "\n",
    "            print(\" - \".join(log_message_parts))\n",
    "            q.task_done()\n",
    "\n",
    "    monitor_task = asyncio.create_task(progress_monitor(progress_queue))\n",
    "\n",
    "    _notebook_logger.info(\n",
    "        f\"--- Starting Deep Research Task {task_id} (Orchestrator auto_run) ---\",\n",
    "        extra={\"agent_name\": \"System\"},\n",
    "    )\n",
    "    _notebook_logger.info(f\"User Query: {user_query}\", extra={\"agent_name\": \"System\"})\n",
    "\n",
    "    final_report = \"Error: Research process did not complete via Orchestrator auto_run.\"\n",
    "\n",
    "    try:\n",
    "        final_report_message = await orchestrator_agent.auto_run(\n",
    "            initial_request=user_query,\n",
    "            request_context=request_context,\n",
    "            max_steps=max_orchestrator_steps,\n",
    "            max_re_prompts=3,\n",
    "        )\n",
    "        final_report = str(final_report_message)  # auto_run returns a string\n",
    "        _notebook_logger.info(\n",
    "            f\"Orchestrator auto_run completed. Final report preview: {final_report[:200]}...\",\n",
    "            extra={\"agent_name\": \"System\"},\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        _notebook_logger.error(\n",
    "            f\"An error occurred during the orchestrator's auto_run: {e}\",\n",
    "            exc_info=True,\n",
    "            extra={\"agent_name\": \"System\"},\n",
    "        )\n",
    "        final_report = (\n",
    "            f\"Error: An unexpected error occurred during the research process: {e}\"\n",
    "        )\n",
    "    finally:\n",
    "        _notebook_logger.info(\n",
    "            f\"--- Deep Research Task {task_id} Finished (Orchestrator auto_run) ---\",\n",
    "            extra={\"agent_name\": \"System\"},\n",
    "        )\n",
    "        await progress_queue.put(None)\n",
    "        await monitor_task\n",
    "\n",
    "    return str(final_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a583a460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 16:28:24,840 - INFO - [DeepResearchNotebook] [System] --- Starting Deep Research Task deep-research-050aa69b-0781-466a-9740-b9abc45abcc1 (Orchestrator auto_run) ---\n",
      "2025-05-19 16:28:24,841 - INFO - [DeepResearchNotebook] [System] User Query: What are the latest advancements in using synthetic data for training large language models, focusing on efficiency and quality?\n",
      "2025-05-19 16:28:24,843 - DEBUG - [urllib3.connectionpool] [System] Starting new HTTPS connection (1): api.openai.com:443\n",
      "2025-05-19 16:28:31,920 - DEBUG - [urllib3.connectionpool] [System] https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "2025-05-19 16:28:31,930 - DEBUG - [DefaultLogger] [System] [Task:deep-research-050aa69b-0781-466a-9740-b9abc45abcc1] [Interaction:e166f947-3a84-48a2-9e32-74adf1589353] [OrchestratorAgent] Agent 'OrchestratorAgent' did not use JSON markdown block. Attempting to parse entire response as JSON.\n",
      "2025-05-19 16:28:31,931 - DEBUG - [urllib3.connectionpool] [System] Starting new HTTPS connection (1): api.openai.com:443\n",
      "2025-05-19 16:28:35,962 - DEBUG - [urllib3.connectionpool] [System] https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "2025-05-19 16:28:35,964 - DEBUG - [DefaultLogger] [System] [Task:deep-research-050aa69b-0781-466a-9740-b9abc45abcc1] [Interaction:928af3df-7469-4328-94a6-240a419ab498] [OrchestratorAgent] Extracted JSON string from markdown block.\n",
      "2025-05-19 16:28:35,966 - DEBUG - [urllib3.connectionpool] [System] Starting new HTTPS connection (1): api.openai.com:443\n",
      "2025-05-19 16:28:48,184 - DEBUG - [urllib3.connectionpool] [System] https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "2025-05-19 16:28:48,190 - DEBUG - [DefaultLogger] [System] [Task:deep-research-050aa69b-0781-466a-9740-b9abc45abcc1] [Interaction:34807f89-4bbc-4f28-8340-f4c287e413c4] [OrchestratorAgent] Extracted JSON string from markdown block.\n",
      "2025-05-19 16:28:48,191 - DEBUG - [urllib3.connectionpool] [System] Starting new HTTPS connection (1): api.openai.com:443\n",
      "2025-05-19 16:28:49,048 - DEBUG - [urllib3.connectionpool] [System] https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "2025-05-19 16:28:49,050 - DEBUG - [DefaultLogger] [System] [Task:deep-research-050aa69b-0781-466a-9740-b9abc45abcc1] [Interaction:34807f89-4bbc-4f28-8340-f4c287e413c4] [OrchestratorAgent] Agent 'OrchestratorAgent' received response from peer 'RetrievalAgent'.\n",
      "2025-05-19 16:28:49,051 - DEBUG - [urllib3.connectionpool] [System] Starting new HTTPS connection (1): api.openai.com:443\n",
      "2025-05-19 16:28:56,578 - DEBUG - [urllib3.connectionpool] [System] https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "2025-05-19 16:28:56,580 - DEBUG - [DefaultLogger] [System] [Task:deep-research-050aa69b-0781-466a-9740-b9abc45abcc1] [Interaction:bb2535b5-a44a-4537-983b-c84cf2809294] [OrchestratorAgent] Extracted JSON string from markdown block.\n",
      "2025-05-19 16:28:56,581 - DEBUG - [DefaultLogger] [System] [Task:deep-research-050aa69b-0781-466a-9740-b9abc45abcc1] [Interaction:883b2451-6b00-4115-a4e2-9321af89cfb0] [ResearcherAgent] Added referenced message ID 29d8d993-26b9-4c81-b300-8a250dbc170e (Role: assistant) to memory.\n",
      "2025-05-19 16:28:56,582 - DEBUG - [urllib3.connectionpool] [System] Starting new HTTPS connection (1): api.openai.com:443\n",
      "2025-05-19 16:28:57,404 - DEBUG - [urllib3.connectionpool] [System] https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "2025-05-19 16:28:57,409 - DEBUG - [DefaultLogger] [System] [Task:deep-research-050aa69b-0781-466a-9740-b9abc45abcc1] [Interaction:bb2535b5-a44a-4537-983b-c84cf2809294] [OrchestratorAgent] Agent 'OrchestratorAgent' received response from peer 'ResearcherAgent'.\n",
      "2025-05-19 16:28:57,410 - DEBUG - [urllib3.connectionpool] [System] Starting new HTTPS connection (1): api.openai.com:443\n",
      "2025-05-19 16:29:08,677 - DEBUG - [urllib3.connectionpool] [System] https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "2025-05-19 16:29:08,681 - DEBUG - [DefaultLogger] [System] [Task:deep-research-050aa69b-0781-466a-9740-b9abc45abcc1] [Interaction:0e193a4a-275c-4297-9d03-c3df2749a516] [OrchestratorAgent] Extracted JSON string from markdown block.\n",
      "2025-05-19 16:29:08,682 - DEBUG - [urllib3.connectionpool] [System] Starting new HTTPS connection (1): api.openai.com:443\n",
      "2025-05-19 16:29:09,653 - DEBUG - [urllib3.connectionpool] [System] https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "2025-05-19 16:29:09,655 - DEBUG - [DefaultLogger] [System] [Task:deep-research-050aa69b-0781-466a-9740-b9abc45abcc1] [Interaction:0e193a4a-275c-4297-9d03-c3df2749a516] [OrchestratorAgent] Agent 'OrchestratorAgent' received response from peer 'RetrievalAgent'.\n",
      "2025-05-19 16:29:09,657 - DEBUG - [urllib3.connectionpool] [System] Starting new HTTPS connection (1): api.openai.com:443\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Example Usage\n",
    "async def main():\n",
    "    query = \"What are the latest advancements in using synthetic data for training large language models, focusing on efficiency and quality?\"\n",
    "    # Run the task\n",
    "    final_result = await run_deep_research_task(query, max_orchestrator_steps=20)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 30 + \" Final Research Report \" + \"=\" * 30)\n",
    "    print(final_result)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# To run in Jupyter:\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de5c1740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResearcherAgent', 'RetrievalAgent', 'SynthesizerAgent'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orchestrator_agent.allowed_peers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5152af1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
